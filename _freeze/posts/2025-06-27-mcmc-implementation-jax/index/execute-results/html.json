{
  "hash": "84f0715c8443e34e57f1de8f3864be6b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Metropolis-Hastings in JAX\"\ndescription: \"Add a dash of performance to classical statistics\"\nauthor:\n  - name: Alin Morariu\n    #orcid: 0000-0002-5300-3075\n    affiliation: Lancaster University\n    affiliation-url: https://www.lancaster.ac.uk/maths/\ndate: 06-27-2025          # MM-DD-YYYY\ncategories: [probability, python, Bayesian] # self-defined categories\nimage: DSC_8579.jpg\ndraft: false\ncode-annotations: hover\nlightbox: auto\nhtml:\n    code-line-numbers: true\n---\n\n\nOne of my favourite things to do is to translate math to code and code to math. Algorithms tend to be written down on paper in a way that is easy to read but not always obvious to implement. In this post, I'm going to go through the Metropolis-Hastings algorithm and show one way of implementing it. There are many ways, some are slower, some are faster, but this is one is mine.\n\n## Bayesian statistics and MCMC \nThe set up for Bayesian problems goes something like this... *given* some observed data we *assume* it comes from a data generating process (we call this the model), so we want to *estimate* the parameters of the model. Let $\\mathcal{D}$ be the observed data and $\\theta$ be the model parameters and we can write the following: \n\n$$\n\\pi(\\theta \\mid \\mathcal{D}) \\propto p(\\theta) \\, \\mathcal{L}(\\mathcal{D} \\mid \\theta)\n$$\n\nWhere $p(\\theta)$ is called the prior distribution (on the model parameters), $\\mathcal{L}(\\mathcal{D} \\mid \\theta)$ is the likelihood function, and $\\pi(\\theta \\mid \\mathcal{D})$ is the posterior. The goal of MCMC is to explore the probability space that is the posteruor distribution. In this post, the posterior distribution is going to be very simple (so much so that you can work it out by hand) but you can imagine that for larger, more complex models, these posterior distributions will not be \"nice\" to work with. What's cool about MCMC algorithms, is that they will (eventually) be able to generate samples from these complex posterior distributions so we can find the optimal parameter values for out models. \n\n## Metropolis-Hastings \nThe Metropolis-Hastings (MH) algorithm is a fundamental MCCM method designed to generate samples from complex probability distributions, such as our posterior. It works when direct sampling isn't possible or when only an unnormalized density function is available. The algorithm goes something like this: \n\nAt each step $t$, the algorithm proposes a new state $\\theta^*$ from a proposal distribution $q(\\theta^*|\\theta_t)$, which often depends on the current state $\\theta_t$. The proposed state is then accepted with a probability $\\alpha$, known as the Metropolis-Hastings acceptance ratio:\n\n$$\n\\alpha(\\theta_t, \\theta^*) = \\min\\left(1, \\frac{\\pi(\\theta^*)q(\\theta_t|\\theta^*)}{\\pi(\\theta_t)q(\\theta^*|\\theta_t)}\\right)\n$$\n\nIf the proposed state is accepted, $\\theta_{t+1} = \\theta^*$; otherwise, the chain remains at its current state, $\\theta_{t+1} = \\theta_t$[^1]. This acceptance criterion is crucial for correcting any bias introduced by the proposal distribution, ensuring that the generated Markov chain has $\\pi(\\theta)$ as its stationary distribution. This outlines a procedure that we can easily write into a `for` loop; however that style of implementation tends to be slow and cumbersome.\n\n:::{.callout-note}\nIf you'd like to see the algorithm, I'd recommend checking out [Scalable Monte Carlo for Bayesian Learning](https://arxiv.org/abs/2407.12751). \n:::\n\n## Data simulation\n\n::: {#4756ee44 .cell execution_count=1}\n``` {.python .cell-code}\nimport jax\nimport jax.numpy as jnp\nimport distrax\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time \n\nfrom typing import Callable, Tuple\n```\n:::\n\n\nWe need some observed data so we will simulate it from a 1-dimensional Gaussian distribution.\n\n::: {#6317e14e .cell execution_count=2}\n``` {.python .cell-code}\nrng_key = jax.random.PRNGKey(20250627) # use a fixed seed for reproducibility\n\n# define the parameters of our *true* data-generating model\n# true_mean is what we are aiming to \"discover\"/estimate through MCMC\ntrue_mean = jnp.array(4.2, dtype=jnp.float32)\ntrue_stddev = jnp.array(1.5, dtype=jnp.float32)\n\n# this model represents the process that generates the observed data.\ndef my_model(mean, stddev):\n    return distrax.Normal(loc=mean, scale=stddev)\n\nmodel = my_model(mean = true_mean, stddev= true_stddev)\n\nobserved_data = model.sample(seed = rng_key, sample_shape=(40,))\n```\n:::\n\n\n::: {#ea153175 .cell execution_count=3}\n``` {.python .cell-code}\n# plot the observed data - think of this as exploratory data analysis\nplt.figure(figsize=(9,4))\nsns.histplot(observed_data, bins=30, kde=False, stat='density', color='green', label='Observed Data Density', alpha=0.6)\nplt.title('Histogram of Observed Data')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.grid(True, linestyle=':', alpha=0.6)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=738 height=376}\n:::\n:::\n\n\n:::{.callout-important}\n**Problem statement**: given the data above, estimate the mean of the data generating model given the standard deviation is 1.5.\n$$ \n\\mathcal{D} \\sim N(\\mu, 1.5^2) \\quad \\leftarrow \\text{Find } \\mu\n$$ \n:::\n\nNow we can create a function that builds our target log-probability function (i.e. the posterior).\n\n:::{.callout-note}\nIn a data science workflow, we won't know for sure that our model is correct since the data is not simulated. So from here on, we will pretend that we do not *know* the data is normal, but instead *assume* that it is. \n:::\n\n::: {#bc7b437c .cell execution_count=4}\n``` {.python .cell-code}\n# create the flat prior distribution (Normal(0, 10^2) for the parameter mu)\nprior_mean = jnp.array(0.0, dtype=jnp.float32)\nprior_stddev = jnp.array(10.0, dtype=jnp.float32)\n\nprior_distribution = distrax.Normal(loc=prior_mean, scale=prior_stddev)\n```\n:::\n\n\n::: {#91a6d44c .cell execution_count=5}\n``` {.python .cell-code}\ndef make_target_log_prob_fn(prior_dist: distrax.Distribution, \n                                  obs_data: jax.Array, \n                                  obs_likelihood_stddev: jax.Array):\n  \"\"\"\n  Factory function to create a callable for the target log-probability (posterior).\n  This function forms a closure over the prior distribution, observed data,\n  and the likelihood standard deviation.\n\n  Args:\n    prior_dist (distrax.Distribution): The prior distribution for the parameter mu.\n    obs_data (jax.Array): The observed data.\n    obs_likelihood_stddev (jax.Array): The standard deviation for the data likelihood.\n\n  Returns:\n    A callable function `target_log_prob_fn(x)` that returns the unnormalized\n    log-posterior probability of mu given the observed data.\n  \"\"\"\n  @jax.jit # JIT compile the log_prob function\n  def target_log_prob_fn(x):\n    \"\"\"\n    Calculates the unnormalized log-posterior probability for a given parameter value 'x'.\n    This is proportional to log(prior(x)) + log(likelihood(observed_data | x)).\n    \"\"\"\n    # calculate the log-probability of 'x' under the prior distribution\n    log_prior = prior_dist.log_prob(x)\n\n    # define the likelihood distribution for the observed data, assuming 'x' is the mean\n    likelihood_dist = distrax.Normal(loc=x, scale=obs_likelihood_stddev)\n\n    # calculate the log-probability of the observed data under this likelihood\n    # and sum over all data points (assuming independence).\n    log_likelihood = jnp.sum(likelihood_dist.log_prob(obs_data))\n\n    # unnormalized log-posterior is the sum of the log-prior and log-likelihood.\n    return log_prior + log_likelihood\n  return target_log_prob_fn\n```\n:::\n\n\nLet's test that it works. \n\n::: {#f3d4dbc9 .cell execution_count=6}\n``` {.python .cell-code}\n# create the specific target_log_prob_callable using our defined prior, observed data, and likelihood stddev.\ntarget_log_prob_fn = make_target_log_prob_fn(\n    prior_distribution, observed_data, true_stddev)\n\n# test the target_log_prob_callable function at a few points.\n# `prior_mean` (0.0) is a good test point for the parameter 'x'.\nprint(f\"Log-posterior at prior mean ({prior_mean.item():.2f}): {target_log_prob_fn(prior_mean).item():.4f}\")\nprint(f\"Log-posterior at 6.0: {target_log_prob_fn(jnp.array(6.0)).item():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLog-posterior at prior mean (0.00): -229.0388\nLog-posterior at 6.0: -101.6034\n```\n:::\n:::\n\n\n## A modular solution\nThe goal in this implementation is to \"section\" off parts of the algorithm that we can allocate to a function that does one thing (a specialist of sorts). The most obvious candidate for this is the proposal function $Q(x'|x; \\sigma)$. \n\n::: {#21ca9815 .cell execution_count=7}\n``` {.python .cell-code}\nproposal_stddev = jnp.array(1.0, dtype=jnp.float32)\n\ndef make_proposal_distribution_fn(proposal_stddev_val):\n  \"\"\"\n  Factory function to create a callable that generates a Normal proposal distribution.\n\n  Args:\n    proposal_stddev_val (jax.Array): The standard deviation for the proposal distribution.\n\n  Returns:\n    A callable function `proposal_dist_fn(center_val)` that returns a\n    distrax.Normal object centered at `center_val` with\n    the given `proposal_stddev_val`.\n  \"\"\"\n  def proposal_dist_fn(center_val):\n    \"\"\"\n    Creates a Normal distribution centered at `center_val`.\n    \"\"\"\n    return distrax.Normal(loc=center_val, scale=proposal_stddev_val)\n  return proposal_dist_fn\n\n# Create the specific proposal distribution callable using our global standard deviation.\nproposal_distribution_fn = make_proposal_distribution_fn(proposal_stddev)\n```\n:::\n\n\nSince the algorithm requires several probability evaluations, I liked the idea of bundling up these calculations into one function (this can be better implemented by having an individual function for each probability since that would be an easier system to unit test).\n\n::: {#451d92ab .cell execution_count=8}\n``` {.python .cell-code}\ndef make_compute_mh_log_probabilities(target_log_prob_fn, proposal_dist_fn):\n    def compute_mh_log_probabilities(current_sample, proposed_sample):\n        \"\"\"\n        Computes all necessary log probabilities for the Metropolis-Hastings acceptance ratio.\n        This function implicitly accesses target_log_prob_fn and proposal_dist_fn from the closure.\n\n        Args:\n            current_sample (jax.Array): The current value in the MCMC chain.\n            proposed_sample (jax.Array): The proposed candidate value.\n\n        Returns:\n            A tuple containing:\n            - proposed_target_log_prob (jax.Array): Log-prob of proposed_sample under target.\n            - current_target_log_prob (jax.Array): Log-prob of current_sample under target.\n            - log_proposal_forward_prob (jax.Array): Log-prob of proposing proposed_sample from current_sample.\n            - log_proposal_reverse_prob (jax.Array): Log-prob of proposing current_sample from proposed_sample.\n        \"\"\"\n        # Calculate the log-probability of the proposed sample under the target distribution.\n        proposed_target_log_prob = target_log_prob_fn(proposed_sample)\n\n        # Calculate the log-probability of the current sample under the target distribution.\n        current_target_log_prob = target_log_prob_fn(current_sample)\n\n        # Define the proposal distribution for the forward step (current -> proposed)\n        proposal_forward_dist = proposal_dist_fn(current_sample)\n        log_proposal_forward_prob = proposal_forward_dist.log_prob(proposed_sample)\n\n        # Define the proposal distribution for the reverse step (proposed -> current)\n        proposal_reverse_dist = proposal_dist_fn(proposed_sample)\n        log_proposal_reverse_prob = proposal_reverse_dist.log_prob(current_sample)\n\n        return (proposed_target_log_prob, current_target_log_prob,\n                log_proposal_forward_prob, log_proposal_reverse_prob)\n        \n    return compute_mh_log_probabilities\n    \ncompute_mh_log_probabilities = make_compute_mh_log_probabilities(\n    target_log_prob_fn, \n    proposal_distribution_fn\n    )\n```\n:::\n\n\nAnd finally for the algorithm! \n\n::: {#cb996c63 .cell execution_count=9}\n``` {.python .cell-code}\ndef make_metropolis_hastings_step(target_log_prob_fn, proposal_dist_fn):\n  \"\"\"\n  Factory function to create a Metropolis-Hastings step function with a closure\n  over the target log-probability function and the proposal distribution callable.\n\n  Args:\n    target_log_prob_fn (callable): A function that returns the log-probability\n                                    of a sample under the target distribution.\n    proposal_dist_fn (callable): A function `(center_val) -> distrax.Normal`\n                                 that generates the proposal distribution centered at `center_val`.\n\n  Returns:\n    A callable `jax.jit` compiled function that performs one step of the Metropolis-Hastings algorithm.\n  \"\"\"\n  # JIT compile the main step function\n  @jax.jit\n  def metropolis_hastings_step(carry, x): # x is a dummy variable from jax.lax.scan's `xs`\n    \"\"\"\n    Performs one step of the Metropolis-Hastings algorithm.\n    This function now uses the target_log_prob_fn and proposal_dist_fn from its closure.\n\n    Args:\n      carry: A tuple containing (current_sample, current_log_prob, rng_key).\n             - current_sample (jax.Array): The current value in the MCMC chain.\n             - current_log_prob (jax.Array): The log-probability of the current_sample\n                                             under the target distribution.\n             - rng_key (jax.Array): The JAX PRNG key for random operations.\n      x: A dummy variable from the `elems` sequence of `jax.lax.scan` (unused here).\n\n    Returns:\n      A tuple (next_carry, output_value) for `jax.lax.scan`:\n        - next_carry: (next_sample, next_log_prob, updated_rng_key) for the next iteration.\n        - output_value: next_sample (the actual sample to be collected).\n    \"\"\"\n    current_sample, current_log_prob, rng_key = carry\n\n    # Split the RNG key for distinct random operations within this step\n    proposal_key, uniform_key, next_rng_key = jax.random.split(rng_key, 3)\n\n    # 1. Propose a new candidate sample using the `proposal_dist_fn` from closure.\n    # Pass the proposal_key for reproducibility\n    proposed_sample = proposal_dist_fn(current_sample).sample(seed=proposal_key)\n\n    # 2. Compute all necessary log probabilities using our nested helper function.\n    (proposed_target_log_prob, _, # We already have current_log_prob from current_state, so ignore this return\n     log_proposal_forward_prob, log_proposal_reverse_prob) = \\\n        compute_mh_log_probabilities(current_sample, proposed_sample)\n\n    # 3. Calculate the Metropolis-Hastings acceptance ratio in log-space\n    log_acceptance_ratio = (proposed_target_log_prob - current_log_prob) + \\\n                           (log_proposal_reverse_prob - log_proposal_forward_prob)\n\n    # The acceptance ratio 'alpha' must be between 0 and 1.\n    acceptance_ratio = jnp.exp(jnp.minimum(0.0, log_acceptance_ratio))\n\n    # 4. Generate a uniform random number for acceptance check\n    # Pass the uniform_key for reproducibility\n    u = jax.random.uniform(uniform_key, shape=current_sample.shape, dtype=current_sample.dtype)\n\n    # 5. Decide whether to accept the proposed sample\n    accept = jnp.less(u, acceptance_ratio)\n\n    # Select the next sample based on acceptance\n    next_sample = jnp.where(accept, proposed_sample, current_sample)\n\n    # Select the log-probability corresponding to the next sample.\n    next_log_prob = jnp.where(accept, proposed_target_log_prob, current_log_prob)\n\n    # Return the new carry state (for next iteration) and the sample to collect\n    return (next_sample, next_log_prob, next_rng_key), next_sample\n\n  return metropolis_hastings_step\n\n```\n:::\n\n\n## Running the algorithm \n\n::: {#f1d26718 .cell execution_count=10}\n``` {.python .cell-code}\n# Set the total number of samples to generate\nnum_samples = 5000\n\n# Define the initial state (starting point) of our MCMC chain.\ninitial_sample = jnp.array(0.0, dtype=jnp.float32)\ninitial_log_prob = target_log_prob_fn(initial_sample)\n\n# The initial_state (carry) for jax.lax.scan is a tuple of (initial_sample, initial_log_prob, initial_rng_key).\ninitial_carry = (initial_sample, initial_log_prob, rng_key)\n```\n:::\n\n\n::: {#9644a7a3 .cell execution_count=11}\n``` {.python .cell-code}\n# Create the specific MH step function (which is already JIT-compiled by its factory)\nmh_step_function = make_metropolis_hastings_step(\n    target_log_prob_fn, proposal_distribution_fn)\n```\n:::\n\n\n::: {#67fac632 .cell execution_count=12}\n``` {.python .cell-code}\n%%time \nprint(f\"Starting Metropolis-Hastings sampling for {num_samples} steps...\")\n\n# Use jax.lax.scan to run the Metropolis-Hastings steps iteratively.\n# jax.lax.scan returns (final_carry, accumulated_outputs)\nfinal_carry, mh_samples_jax = jax.lax.scan(\n    f=mh_step_function,\n    init=initial_carry,\n    xs=jnp.arange(num_samples) # A dummy sequence of length num_samples to drive the iterations\n)\n\nprint(\"Sampling complete!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStarting Metropolis-Hastings sampling for 5000 steps...\nSampling complete!\nCPU times: user 460 ms, sys: 20.5 ms, total: 480 ms\nWall time: 120 ms\n```\n:::\n:::\n\n\n## Visualizing the results\n\n::: {#a69df39c .cell execution_count=13}\n``` {.python .cell-code}\nmh_samples = np.array(mh_samples_jax) \n```\n:::\n\n\n::: {#26c50bec .cell execution_count=14}\n``` {.python .cell-code}\nplt.figure(figsize=(9,4))\nplt.plot(mh_samples, color='blue', alpha=0.6)\n# Plot the true mean of the data-generating process\nplt.axhline(true_mean.item(), color='red', linestyle='--', linewidth = 2, label=f'True Mean (Data Generating) ({true_mean.item():.2f})')\n# Plot the estimated mean from the latter part of the MCMC chain (after some burn-in)\nplt.axhline(np.mean(mh_samples[500:]), color='orange', linestyle='--', linewidth = 2, label=f'Estimated Mean (Post Burn-in) ({np.mean(mh_samples[500:]):.2f})')\nplt.title('Metropolis-Hastings Trace Plot')\nplt.xlabel('Iteration')\nplt.ylabel('Sample Value')\nplt.grid(True, linestyle=':', alpha=0.6)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){width=725 height=376}\n:::\n:::\n\n\n::: {#e3a22427 .cell execution_count=15}\n``` {.python .cell-code}\nplt.figure(figsize=(9,4))\nsns.histplot(mh_samples, bins=50, kde=True, stat='density', color='green', label='MH Samples Density', alpha=0.6)\n\n# Generate points for the prior PDF\nx_range = np.linspace(np.min(mh_samples) - 1, np.max(mh_samples) + 1, 500)\n# Convert JAX array from distrax.prob to NumPy for plotting\nprior_pdf = np.array(prior_distribution.prob(jnp.array(x_range)))\nplt.plot(x_range, prior_pdf, color='red', linestyle='--', label='Prior PDF')\n\nplt.title('Distribution of MH Samples vs. Prior PDF')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True, linestyle=':', alpha=0.6)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-1.png){width=738 height=376}\n:::\n:::\n\n\n::: {#ce31d535 .cell execution_count=16}\n``` {.python .cell-code}\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten() # Flatten the 2x2 array of axes for easy iteration\n\niteration_points = [10, 100, 1000, 5000]\n\n# The true posterior calculations are removed as they are rarely available in practice.\n# We will compare the MCMC estimates against the true data-generating mean.\n# We use the 'true_mean' (4.2) as a reference point for the parameter.\n\nx_range = np.linspace(np.min(mh_samples) - 1, np.max(mh_samples) + 1, 500) # Keep range based on samples\n\n\nfor i, num_iters in enumerate(iteration_points):\n    # Slice the samples up to the current iteration point\n    current_samples = mh_samples[:num_iters]\n\n    ax = axes[i]\n    sns.kdeplot(current_samples, ax=ax, color='blue', fill=True, label='MCMC Estimate')\n    # Plot a vertical line at the true data-generating mean for reference\n    ax.axvline(true_mean.item(), color='red', linestyle='--', label='True Data Mean')\n    ax.set_title(f'KDE after {num_iters} Iterations')\n    ax.set_xlabel('Parameter Value (X)')\n    ax.set_ylabel('Density')\n    ax.legend()\n    ax.grid(True, linestyle=':', alpha=0.6)\n\nplt.suptitle('Evolution of Parameter Distribution with MCMC Iterations', fontsize=16, y=0.98)\nplt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to prevent title overlap\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){width=1142 height=946}\n:::\n:::\n\n\nI really like this plot because it shows the intuition behind the algorithm. A well tuned algorithm will slowly converge towards the true underlying parameter value and overpower a flat, uninformative prior. \n\n::: {#062a0de3 .cell execution_count=17}\n``` {.python .cell-code}\nprint(\"\\n--- Sample Statistics ---\")\nprint(f\"Mean of MH samples: {np.mean(mh_samples):.4f}\")\nprint(f\"Standard deviation of MH samples: {np.std(mh_samples):.4f}\")\n# Use .item() for scalar JAX arrays when printing\nprint(f\"True Mean: {true_mean.item():.4f}\")\n\n# Calculate differences between consecutive samples\ndiffs = np.diff(mh_samples)\n# Count where the difference is not zero (i.e., a new sample was accepted)\naccepted_steps = np.sum(diffs != 0)\n# Acceptance rate is the number of accepted steps divided by total steps (excluding initial)\nacceptance_rate = accepted_steps / (num_samples - 1) # Subtract 1 because diff reduces length by 1\n\nprint(f\"Approximate Acceptance Rate: {acceptance_rate:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n--- Sample Statistics ---\nMean of MH samples: 4.1865\nStandard deviation of MH samples: 0.2855\nTrue Mean: 4.2000\nApproximate Acceptance Rate: 0.2845\n```\n:::\n:::\n\n\n### Thanks for reading\nIn my spare time, I like to take photos so I'm going to add one photo I like at the end of each post as a thank you :) \n![Lake District, UK, 2024](DSC_8579.jpg){.lightbox}\n\n## Footnotes \n[^1]: If you want a formal description of the algorithm, you can find it on the [Wikepedia page](https://en.wikipedia.org/wiki/Metropolis–Hastings_algorithm).\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}