[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nConditional probabilities are partial function evaluations\n\n\n7 min\n\n\n\nprobability\n\n\npython\n\n\n\nI’m going to try to convince you that conditional probabilities are easy.\n\n\n\nSep 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating an epidemic\n\n\n10 min\n\n\n\nepidemics\n\n\nsimulation\n\n\n\nWe take a look at the popular (and simple) chain-binomial model for simulating an epidemic\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alin Morariu",
    "section": "",
    "text": "Github\n  \n  \n    \n     Google Scholar\n  \n\n  \n  \n\nprint(my_intro)\n\nHi, I’m Alin! Welcome to my website. I will be using this site as a way to display some of my work and interests. You can find some of my projects, talks, and the occasional blog post about stuff I’ve found cool and thought was worth sharing.\n\n\nI am a PhD student at Lancaster University in the UK, currently supervised by Prof. Chris Jewell and Prof. Paul Fearnhead. My research focuses on computational challenges for fitting stochastic epidemic models, specifically MCMC algorithms spatio-temporal models for spread of disease."
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Alin Morariu",
    "section": "Research interests",
    "text": "Research interests\n\n\n\n\n\n\nEpidemic modelling\n\n\n\n\n\nEpidemic models provide useful insight into how we can manage the spread of disease in both human and animal populations. They can be powerful tools for public policy and pose interesting computational challenges.\n\n\n\n\n\n\n\n\n\nLikelihood inference and MCMC\n\n\n\n\n\nBayesian methods provide robust estimation for complex models where analytical solutions are often infeasible. They allow for exploration of high-dimensional parameter spaces\n\n\n\n\n\n\n\n\n\nParallel and high performance computing\n\n\n\n\n\nOne of the best ways to increase the efficiency of fitting models is to take advantage of modern hardware like GPUs and HPC clusters. This can be done by implementing algorithms so they can be run in parallel across more than one device."
  },
  {
    "objectID": "index.html#personal-and-hobbies",
    "href": "index.html#personal-and-hobbies",
    "title": "Alin Morariu",
    "section": "Personal and hobbies",
    "text": "Personal and hobbies\n\n\n\n\n\n\nCycling\n\n\n\n\n\nA very famous man once said “I like to ride my bicycle” and I think he was on to something there.\n\n\n\n\n\n\n\n\n\nPhotography\n\n\n\n\n\nI take picture… mostly with a very little camera."
  },
  {
    "objectID": "talks/2024-01-31-Pydata-software-banks/index.html",
    "href": "talks/2024-01-31-Pydata-software-banks/index.html",
    "title": "From code banks to software",
    "section": "",
    "text": "As coders, we often build up vast amounts of code that can be used repeatedly and potentially shared with others to make their lives easier. How can we turn those banks of code into libraries? This talk will explore the journey I took to becoming a software developer and the lessons I learned along the way as someone who was not a computer scientist first."
  },
  {
    "objectID": "talks/2024-01-31-Pydata-software-banks/index.html#abstract",
    "href": "talks/2024-01-31-Pydata-software-banks/index.html#abstract",
    "title": "From code banks to software",
    "section": "",
    "text": "As coders, we often build up vast amounts of code that can be used repeatedly and potentially shared with others to make their lives easier. How can we turn those banks of code into libraries? This talk will explore the journey I took to becoming a software developer and the lessons I learned along the way as someone who was not a computer scientist first."
  },
  {
    "objectID": "talks/2024-01-31-Pydata-software-banks/index.html#slides",
    "href": "talks/2024-01-31-Pydata-software-banks/index.html#slides",
    "title": "From code banks to software",
    "section": "Slides",
    "text": "Slides\n\n\n\n\n\n Full Screen"
  },
  {
    "objectID": "posts/2024-09-30-conditional-probs-partial-evals/index.html",
    "href": "posts/2024-09-30-conditional-probs-partial-evals/index.html",
    "title": "Conditional probabilities are partial function evaluations",
    "section": "",
    "text": "During my undergraduate degree, I always struggled with the concept of computing statistical quantities. I couldn’t wrap my head around what commands to use in my editor to mimic the math that I could write on the page. Fast forward a several years and lots of coding assignments, I now kind of get it. I’m writing this blog post to highlight one of the nicest links between the math and the computation that I’ve come across and the motivation behind the title of this post - conditional probabilities are partial function evaluations."
  },
  {
    "objectID": "posts/2024-09-30-conditional-probs-partial-evals/index.html#quick-refresher-on-conditional-probabilities",
    "href": "posts/2024-09-30-conditional-probs-partial-evals/index.html#quick-refresher-on-conditional-probabilities",
    "title": "Conditional probabilities are partial function evaluations",
    "section": "Quick refresher on conditional probabilities",
    "text": "Quick refresher on conditional probabilities\nLet’s start with a quick recap of what exactly a conditional probability is. Suppose we have 2 events, \\(A\\) and \\(B\\). We say that the conditional probability of B given A is \\[P(B \\vert A) = \\frac{P(A \\bigcap B)}{P(A)}\\] We assume that \\(P(A)&gt;0\\). The intuition here is that we are updating our belief about \\(B\\) knowing that something specific about \\(A\\) has occured. If these events are independent, then the above equation simplifies since we \\(P(A \\bigcap B) = P(A) P(B)\\) (i.e. knowing something about \\(A\\) gives you no additional information about \\(B\\)).\nHowever, conditional probability extends far beyond simple events. In statistical modeling, conditional probabilities form the backbone of likelihood-based inference. When we model data, we often use conditional probabilities to express how likely the observed data is, given a set of model parameters (data generating model). This is where the likelihood function comes into play. The likelihood function is an unnormalized probability distribution that is a function of the model parameters, not the data. It captures the plausibility of the parameters given the data, and by conditioning on different parameters, we can explore various hypotheses or refine our model.\nFor example, in Bayesian inference, we condition on observed data to update our prior beliefs about the model parameters, yielding the posterior distribution. This conditional framework underpins nearly all of modern statistical inference, from maximum likelihood estimation to more complex Markov Chain Monte Carlo inference schemes. These likelihood functions can be very easy to write on paper but difficult to code and that’s what I want to dive into here. Let’s define a model with two parameters. \\[y_i \\sim N(\\beta_0 + \\beta_1 x_i, 2^2)\\] Some may recognize this as a linear regression. Our dependent random varialbe \\(y\\) follows a Normal distribution which has a mean that depends on a linear transformation of the independent variable \\(x\\). We can write out the likelihood function of this model using the probability density function of the Normal. \\[\\begin{align}\nL(\\beta_0, \\beta_1 ; x) &= \\prod_{i = 1}^n P(y_i) \\\\\n&= \\prod_{i = 1}^n \\frac{1}{\\sqrt{2 \\pi \\cdot0.52^2}} e^{-\\frac{(y_i - (\\beta_0 + \\beta_1 x_i))^2}{2 \\cdot 0.5^2}}\n\\end{align}\\] Here’s a simulated data set from this model.\n\n# parameter values \nbeta0 = 1.2\nbeta1 = 0.5\n\n# random x values\nx = np.random.uniform(\n    low = 0.0, \n    high = 10.0,\n    size = 100,\n    )\n# evaluate mean\nmu = beta0 + beta1*x\n# simulate y values \ny = np.random.normal(\n    loc = mu,\n    scale = 0.5, \n    size = 100\n    )\n\n\n# Scatter plot of the data\nplt.scatter(x, y, label=\"Data points\", color='blue')"
  },
  {
    "objectID": "posts/2024-09-30-conditional-probs-partial-evals/index.html#partial-function-evaluations",
    "href": "posts/2024-09-30-conditional-probs-partial-evals/index.html#partial-function-evaluations",
    "title": "Conditional probabilities are partial function evaluations",
    "section": "Partial Function Evaluations",
    "text": "Partial Function Evaluations\nIn Python, partial function evaluation is a feature provided by functools package with the partial function. It allows you to take a function and fix some of its arguments, returning a new function that takes fewer arguments. The similarity to conditional probabilities lies in this idea of fixing known inputs. This is a nice feature of Python - we can pass around functions as first class objects (an entity that can be dynamically created, destroyed, passed to a function, returned as a value, and have all the rights as other variables in the programming language have).\nLet’s look at an example.\n\nfrom functools import partial \n\n\ndef my_function(a, b):\n    \"\"\"\n    A function that takes two arguements and returns the product\n    \"\"\"\n    return a*b\n\nWe can partially evaluate the function by “fixing” one of the parameters values to a specific value. For example, fix a=2.\n\nmultiply_by_2 = partial(my_function, a = 2)\n\nprint(multiply_by_2)                        # this returns a function\nprint(multiply_by_2(b = 4))                 # this returns 8\n\nfunctools.partial(&lt;function my_function at 0x1186877e0&gt;, a=2)\n8\n\n\nHere we used partial to create a new function that fixes one of the arguments to 2, hence the new function is multiplies the input by 2. If the arguments to the my_function correspond to parameter values in my model and the output is the likelihood function, then applying partial returns the conditional likelihood."
  },
  {
    "objectID": "posts/2024-09-30-conditional-probs-partial-evals/index.html#why-does-this-analogy-work",
    "href": "posts/2024-09-30-conditional-probs-partial-evals/index.html#why-does-this-analogy-work",
    "title": "Conditional probabilities are partial function evaluations",
    "section": "Why does this analogy work?",
    "text": "Why does this analogy work?\nThe analogy between partial function evaluation and conditional probability is very nice because both involve reducing complexity by “conditioning” on known information. In statistics, we’re conditioning on known events/realizations, while in programming, we’re fixing known inputs/data.\nConsider this: when you fix one argument of a function, you’re effectively “conditioning” the function on that known value. Similarly, conditional probability refines the likelihood of an event by fixing certain known information. This way of thinking can be particularly powerful when designing simulations or modeling problems where we frequently update our beliefs based on new information."
  },
  {
    "objectID": "posts/2024-09-30-conditional-probs-partial-evals/index.html#apply-it-to-our-model",
    "href": "posts/2024-09-30-conditional-probs-partial-evals/index.html#apply-it-to-our-model",
    "title": "Conditional probabilities are partial function evaluations",
    "section": "Apply it to our model",
    "text": "Apply it to our model\nLets create the likelihood function of our model. I’m going to take advantage of the logpdf method of the normal distribution which ‘scipy’ has already implemented. It’s always good practice to allocate these computations to well tested and documented libraries so we avoid any algebraic mistakes in our code.\n\ndef model_likelihood(beta0, beta1):\n    log_probs = sp.stats.norm(\n        loc = mu,           # model mean - computed as global variable earlier\n        scale = 0.5         # fixed variance\n        ).logpdf(y)         # log prob of observed random variables \n    return np.sum(log_probs)\n\n# test arbitrary values\nprint(f\"log-prob eval: {model_likelihood(beta0 = 0.1, beta1 = 0.1)}\")\n\nlog-prob eval: -66.95622048686633\n\n\nNow we can use the partial to get the conditional likelihood of beta0 given beta1.\n\nconditional_likelihood = partial(model_likelihood, beta1 = 0.1)\n\nprint(f\"Conditional log-likelihood: {conditional_likelihood}\")\nprint(f\"Check: {conditional_likelihood(beta0= 0.1)}\")\n\nConditional log-likelihood: functools.partial(&lt;function model_likelihood at 0x118687600&gt;, beta1=0.1)\nCheck: -66.95622048686633\n\n\nAnd they are the same as before! This mirrors how we compute conditional probabilities by progressively refining our estimate as more information becomes available. For example, if you found some information saying that beta1 should be 0.5, you can fix that using this technique. All you need to do then is optimize the (log) likelihood for the other parameter."
  },
  {
    "objectID": "posts/2024-09-30-conditional-probs-partial-evals/index.html#closing-thoughts",
    "href": "posts/2024-09-30-conditional-probs-partial-evals/index.html#closing-thoughts",
    "title": "Conditional probabilities are partial function evaluations",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nThe parallel between partial function evaluations and conditional probabilities provides an intuitive bridge between coding and probability theory. By conditioning on known values, both in probability and programming, we can simplify complex systems and gain clearer insights into the behavior of the remaining uncertainties.\nIn your next coding or probability problem, try thinking of how partial evaluations might represent conditioned states of knowledge. This perspective can make otherwise complex ideas feel a bit more manageable—and highlight the deep connections between computation and probability theory.\n\nThanks for reading\nIn my spare time, I like to take photos so I’m going to add one photo I like at the end of each post as a thank you :)"
  },
  {
    "objectID": "posts/2024-07-01-simulating-epidemics/index.html",
    "href": "posts/2024-07-01-simulating-epidemics/index.html",
    "title": "Simulating an epidemic",
    "section": "",
    "text": "When I started my PhD two years ago, I had never looked at an epidemic model outside of an elementry differential equations course. Fast forward two years, and one Covid-19 later, the bulk of my work is centred around epidemic models. More specifically, stochastic epidemic models. Differential equation models are simple and easy to use but are often too rigid since they don’t reflect the randomness we see in the real world when it comes to the spread of disease.\nThis post is going to be a tutorial that I wish I had at the start of my epidemic modelling journey. The aim is to implement/code a chain binomial epidemic model in Python. This model is sometimes referred to as the Reed-Frost model and was initially used to model epidemic spread in the late 1920s (more details1). It is one of the simplest epidemic models as it only focuses on the infection process so take this as the base case. Future posts will dive deeper into the mathematics and how to expand on this model to make it more realistic."
  },
  {
    "objectID": "posts/2024-07-01-simulating-epidemics/index.html#reed-frost-model",
    "href": "posts/2024-07-01-simulating-epidemics/index.html#reed-frost-model",
    "title": "Simulating an epidemic",
    "section": "Reed-Frost model",
    "text": "Reed-Frost model\nThe Reed-Frost model helps us predict the number of people who will become infected over time, given some initial conditions.\n\nIntuition\nImagine a group of people where some are initially infected with a disease, and others are susceptible but not yet infected. The Reed-Frost model works by dividing time into discrete steps2, such as days or weeks. At each time step, each susceptible person has a chance of getting infected based on their contact with infectious individuals.\nAt the beginning of the outbreak, the population is divided into two groups: susceptible (those who can get infected) and infectious (those who are currently infected). If we were to think of this as a graphical representation, it would be a 2 compartment model.\n\n\n\n\n\nflowchart LR\n  A(Susceptible) --&gt; B(Infected)\n\n\n\n\n\n\nNow the question becomes how do individuals move between these states. The model uses a probability parameter to represent the likelihood that a susceptible person will get infected upon contact with an infectious person. This probability is often denoted as \\(p\\).\nThe process unfolds over a series of discrete time steps. At each step, every susceptible individual has a chance to become infected if they come into contact with any infectious individuals (this is a very loose way to apply some mathematical structure to a very complex infection transmission mechanism).\nThe number of new infections at each time step depends on the number of susceptible and infectious individuals and the probability of transmission. The model assumes that once a person is infected, they remain infectious for only one time step.\n\n\nWhy this is a good starting\nThe Reed-Frost model is powerful because it captures the essential dynamics of disease transmission in a straightforward manner. It considers the key factors that drive an epidemic: the number of susceptible individuals, the number of infectious individuals, and the probability of transmission. By iterating this process over multiple time steps, the model can simulate the spread of an infection and help predict its potential impact on the population."
  },
  {
    "objectID": "posts/2024-07-01-simulating-epidemics/index.html#python-implementation",
    "href": "posts/2024-07-01-simulating-epidemics/index.html#python-implementation",
    "title": "Simulating an epidemic",
    "section": "Python implementation",
    "text": "Python implementation\nFor this tutorial, I am going to try to use as few packages as possible so I’m going to restrict myself to only numpy and pandas. The rest of the code will be base Python.\n\n# load packages \nimport numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nsns.set()\n\nLet’s take stock of what we know at this point. The Reed-Frost model which is a 2-state compartmental model.\n\n\n\n\n\n\nImportant\n\n\n\nThis is a class of models and not a specific model.\n\n\nTo specify a model we need to add more structure. We want to know the size of population the disease is spreading through, the duration of each step in which events are happening (recall this is a discrete time model), and the last is going to be the transition rate or epidemic dynamics (how the disease is transmitted). The assumptions are as follows:\n\n100 epidemic units3 with 99 susceptibles and 1 infected units. This is represented as a vector \\([S_0, I_0] = [99,1]\\) (the subscript \\(i\\) is the time period).\nTime period is going to be daily so we set the time delta \\(\\triangle d = 1\\).\nEpidemic dynamics will be a density based transmission where we ‘count’ the number of pairwise interactions between infectious units and susceptibles and divide by the population size. This means that the probabilty of an infection happening has a rate of \\(\\lambda_i= \\beta \\frac{S_i I_i}{S_i + I_i}\\) which translates into a probability of \\(p_i = \\exp\\{- \\lambda_i * \\triangle d \\}\\)4.\n\n\n# initial values\ntime_delta = 1.                 # 1 day \ninitial_pop = [99.,1.]          # population vector \nparameters0 = 0.01              # daily transmission parameter \n\nWith the constants and initial conditions set, its time to move to the main bit of the code which performs the simulation.\n\n\n\n\n\n\nTip\n\n\n\nI’ve been using the term implementing a model and this doesn’t really have a set definition. In this case, implementation is done when you can simulate a correct trajectory of the epidemic but in other settings it may mean fitting a model to data, validating, predicting, etc. The scope of an implementation changes with the nature of the problem and is context specific. Its all jargon here.\n\n\nThe function below called SI_iteration takes 3 parameter values: 1. parameters - this is a vector of model parameters. For now, this is a 1D vector holding the value for \\(\\beta\\). 2. state - this is a 2D vector containing the counts for each of the states. 3. time_delta - the size (in days) of the discrete time step\n\ndef SI_iteration(parameters, state, time_delta):\n  beta = parameters\n  num_susceptibles, num_infected = state\n  \n  # Step 1: compute transition rates \n1  SI_rate = beta*(num_susceptibles * num_infected)/(num_susceptibles + num_infected)\n\n  # Step 2: convert event rates to probabilities \n2  i_rate = 1 - np.exp(- SI_rate * time_delta)\n\n  # Step 3: simulate number of new infections \n3  num_new_infections = np.random.binomial(num_susceptibles, i_rate)\n\n  # Step 4; state update\n  return [num_susceptibles - num_new_infections, num_infected + num_new_infections]\n\n\n1\n\nCompute the transition rate\n\n2\n\nTurn the transition rate into a probability of new infections\n\n3\n\nSample the number of new infections\n\n\n\n\nI’m going to ignore the mathematics of Step 2 in chunk above for now since that will be its own post down the line. The gory details can be found in the paper I mentioned earlier for those curious. Now for using our new function! Taking the initial values for each of the arguments and plugging them in, we can get the outcome of a single iteration or single step of our epidemic process. This is not a complete path!\n\n# single iteration/forward step\n1np.random.seed(20240701)\nnew_state = SI_iteration(parameters0, initial_pop, time_delta)\n\nprint(f'New state: {new_state}')\n\n\n1\n\nSet the seed for reproducible value.\n\n\n\n\nNew state: [98.0, 2.0]\n\n\n\nSimulate a path\nWe can extend the single iteration to sample a full path of the epidemic by calling our function over and over again. This is not the most efficient way of performing a simulation but it is easy to understand so it’ll do for now.\n\n# simulate full path\ndef simulate(initial_state, num_steps):\n    # counters\n    ii = 0\n    S = np.zeros(num_steps)\n    I = np.zeros(num_steps)\n\n    # unpack the state\n    S[0], I[0] = initial_state\n    state = initial_state\n\n    while ii &lt; num_steps:\n      # single iteration to update the state\n      state = SI_iteration(parameters0, state, time_delta)\n      # record the outcome\n      S[ii], I[ii] = state\n\n      ii += 1\n\n    return {'Time': np.cumsum(np.repeat(time_delta, num_steps)),\n            'Susceptible': S,\n            'Infected': I}\n\nAnd run a simulation of 100 steps (note that some of the steps might be non-events since every unit has progressed through the epidemic). There are nice ways of stopping simulations early by using a clever stop condition within the while loop.\n\nsample_epi = simulate(initial_pop, 100)\n\npd.DataFrame(sample_epi).head(10)\n\n\n\n\n\n\n\n\nTime\nSusceptible\nInfected\n\n\n\n\n0\n1.0\n99.0\n1.0\n\n\n1\n2.0\n97.0\n3.0\n\n\n2\n3.0\n94.0\n6.0\n\n\n3\n4.0\n86.0\n14.0\n\n\n4\n5.0\n74.0\n26.0\n\n\n5\n6.0\n56.0\n44.0\n\n\n6\n7.0\n41.0\n59.0\n\n\n7\n8.0\n35.0\n65.0\n\n\n8\n9.0\n29.0\n71.0\n\n\n9\n10.0\n21.0\n79.0\n\n\n\n\n\n\n\nLets plot the simulation!\n\nplt.figure(figsize=(12, 7))\n\nS_line = plt.plot(\"Time\",\"Susceptible\", data=sample_epi, color=\"b\", linewidth=2)\nI_line = plt.plot(\"Time\",\"Infected\", data=sample_epi, color=\"r\", linewidth=2)\n\nplt.xlabel(\"Days\",fontweight=\"bold\")\nplt.ylabel(\"Count\",fontweight=\"bold\")\n\nlegend = plt.legend(title=\"Population\",loc=5,bbox_to_anchor=(1.2,0.5))\nframe = legend.get_frame()\nframe.set_facecolor(\"white\")\nframe.set_linewidth(0)\n\n# Add labels and title\nplt.xlabel('Days', fontweight=\"bold\")  # Add x-axis label with font styling\nplt.ylabel('Count', fontweight=\"bold\")  # Add y-axis label with font styling\nplt.title('Single path simulation', fontsize=16)  # Add plot title with font size\n\nplt.show()\n\n\n\n\nFull path of an epidemic. Simulation is done by calling the single iteration function repeatedly\n\n\n\n\n\n\nConclusion\nIn this tutorial, I showed one way to implement a chain binomial epidemic model. This basic model is a great starting point for building more complex and realistic models. The function SI_iteration uses a simplified version of the Gillespie algorithm which is the standard algorithm for simulating stochastic trajectories for these types of systems. I’d also recommend checking out this blog post by Lewis Cole for an in depth look at the algorithm.\n\n\nNext steps\nAs far as using epidemic models in the real world goes, simulation is only one half of the coin. The other side has to do with fitting these models. If the problem was reversed and we had observed an epidemic process, recorded the data and created a data set that looks something like @epi_dataset we could then try to find the value of parameters which generated that data. This can be done in a variety of ways such as maximum likelihood estimation or Bayesian inference. If this type of problem interests you, I’d highly recommend checking out the IDDinf 2024 inference course. This is a course I will be co-teaching with some incredible people in September 2024 and will go over everything you need to know to fit epidemic models with Bayesian inference.\n\n\nThanks for reading\nIn my spare time, I like to take photos so I’m going to add one photo I like at the end of each post as a thank you :)"
  },
  {
    "objectID": "posts/2024-07-01-simulating-epidemics/index.html#footnotes",
    "href": "posts/2024-07-01-simulating-epidemics/index.html#footnotes",
    "title": "Simulating an epidemic",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nResource here↩︎\nI usually work with continuous time models which pose their own challenges. More on this will come in later posts↩︎\nAn epidemic unit can be anything that we can classify as susceptible or infected. These models are used for humans and animals so its easier to generalize the concept of an epidemic unit. The units can even be aggregated to represent households or farms. It gives us a more flexibility to adapt the scope of the model.↩︎\nFuture post on the interpretable mathematics of epidemic models↩︎"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Talks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom code banks to software\n\n\n\n\n\nExploring the lifeline of a coding project and key decisions developers must make along the way\n\n\n\n\n\nJan 31, 2024\n\n\nAlin Morariu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/index_talk_template.html",
    "href": "talks/index_talk_template.html",
    "title": "Talk title",
    "section": "",
    "text": "Your abstract"
  },
  {
    "objectID": "talks/index_talk_template.html#abstract",
    "href": "talks/index_talk_template.html#abstract",
    "title": "Talk title",
    "section": "",
    "text": "Your abstract"
  },
  {
    "objectID": "talks/index_talk_template.html#slides",
    "href": "talks/index_talk_template.html#slides",
    "title": "Talk title",
    "section": "Slides",
    "text": "Slides\n\n\n\n\n\n Full Screen"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a PhD student at Lancaster University with an interest in Bayesian statistics and all the computational aspects that come along with that. This includes inference algorithms like MCMC, probablistic programming, efficient implementations, and software for making this more accessible. In the past, I’ve worked as a data scientist and analyst in the finance industry as well as a researcher in a hospital.\n\n\nLancaster University | Lancaster, UK PhD in Statistics | Jan 2022 - Dec 2025\nToronto Metropolitan Univeristy | Toronto, Canada MSc in Applied Mathematics | Sept 2019 - June 2021\nUniversity of Toronto, St George | Toronto, Canada HBSc in Statistics and Mathematics | Sept 2014 - June 2019\n\n\n\nMy previous work experience can be found on my resume or my LinkedIn"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "Lancaster University | Lancaster, UK PhD in Statistics | Jan 2022 - Dec 2025\nToronto Metropolitan Univeristy | Toronto, Canada MSc in Applied Mathematics | Sept 2019 - June 2021\nUniversity of Toronto, St George | Toronto, Canada HBSc in Statistics and Mathematics | Sept 2014 - June 2019"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "",
    "text": "My previous work experience can be found on my resume or my LinkedIn"
  },
  {
    "objectID": "about.html#get-in-touch",
    "href": "about.html#get-in-touch",
    "title": "About",
    "section": "Get in touch!",
    "text": "Get in touch!\nBest ways of contacting me is through email!\n\\(\\rightarrow\\) a.morariu@lancaster.ac.uk\\(\\leftarrow\\)"
  }
]