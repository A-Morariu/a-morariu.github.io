[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings in JAX\n\n\n15 min\n\n\n\nprobability\n\n\npython\n\n\nBayesian\n\n\n\nAdd a dash of performance to classical statistics\n\n\n\nJun 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConditional probabilities are partial function applications\n\n\n7 min\n\n\n\nprobability\n\n\npython\n\n\n\nI’m going to try to convince you that conditional probabilities are easy.\n\n\n\nSep 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating an epidemic\n\n\n10 min\n\n\n\nepidemics\n\n\nsimulation\n\n\n\nWe take a look at the popular (and simple) chain-binomial model for simulating an epidemic\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alin Morariu",
    "section": "",
    "text": "Github\n  \n  \n    \n     Google Scholar\n  \n\n  \n  \n\nprint(my_intro)\n\nHi, I’m Alin! Welcome to my website. I will be using this site as a way to display some of my work and interests. You can find some of my projects, talks, and the occasional blog post about stuff I’ve found cool and thought was worth sharing.\n\n\nI am a PhD student at Lancaster University in the UK, currently supervised by Prof. Chris Jewell and Prof. Paul Fearnhead. My research focuses on computational challenges for fitting stochastic epidemic models, specifically MCMC algorithms spatio-temporal models for spread of disease."
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Alin Morariu",
    "section": "Research interests",
    "text": "Research interests\n\n\n\n\n\n\nEpidemic modelling\n\n\n\n\n\nEpidemic models provide useful insight into how we can manage the spread of disease in both human and animal populations. They can be powerful tools for public policy and pose interesting computational challenges.\n\n\n\n\n\n\n\n\n\nLikelihood inference and MCMC\n\n\n\n\n\nBayesian methods provide robust estimation for complex models where analytical solutions are often infeasible. They allow for exploration of high-dimensional parameter spaces\n\n\n\n\n\n\n\n\n\nParallel and high performance computing\n\n\n\n\n\nOne of the best ways to increase the efficiency of fitting models is to take advantage of modern hardware like GPUs and HPC clusters. This can be done by implementing algorithms so they can be run in parallel across more than one device."
  },
  {
    "objectID": "index.html#personal-and-hobbies",
    "href": "index.html#personal-and-hobbies",
    "title": "Alin Morariu",
    "section": "Personal and hobbies",
    "text": "Personal and hobbies\n\n\n\n\n\n\nCycling\n\n\n\n\n\nA very famous man once said “I like to ride my bicycle” and I think he was on to something there.\n\n\n\n\n\n\n\n\n\nPhotography\n\n\n\n\n\nI take picture… mostly with a very little camera.\n\n\n\n\n\n\n\n\n\nCoffee\n\n\n\n\n\nEverything is better with a cup of coffee… especially writing a blog post!"
  },
  {
    "objectID": "talks/2024-01-31-Pydata-software-banks/index.html",
    "href": "talks/2024-01-31-Pydata-software-banks/index.html",
    "title": "From code banks to software",
    "section": "",
    "text": "As coders, we often build up vast amounts of code that can be used repeatedly and potentially shared with others to make their lives easier. How can we turn those banks of code into libraries? This talk will explore the journey I took to becoming a software developer and the lessons I learned along the way as someone who was not a computer scientist first."
  },
  {
    "objectID": "talks/2024-01-31-Pydata-software-banks/index.html#abstract",
    "href": "talks/2024-01-31-Pydata-software-banks/index.html#abstract",
    "title": "From code banks to software",
    "section": "",
    "text": "As coders, we often build up vast amounts of code that can be used repeatedly and potentially shared with others to make their lives easier. How can we turn those banks of code into libraries? This talk will explore the journey I took to becoming a software developer and the lessons I learned along the way as someone who was not a computer scientist first."
  },
  {
    "objectID": "talks/2024-01-31-Pydata-software-banks/index.html#slides",
    "href": "talks/2024-01-31-Pydata-software-banks/index.html#slides",
    "title": "From code banks to software",
    "section": "Slides",
    "text": "Slides\n\n\n\n\n\n Full Screen"
  },
  {
    "objectID": "posts/2025-06-27-mcmc-implementation-jax/index.html",
    "href": "posts/2025-06-27-mcmc-implementation-jax/index.html",
    "title": "Metropolis-Hastings in JAX",
    "section": "",
    "text": "One of my favourite things to do is to translate math to code and code to math. Algorithms tend to be written down on paper in a way that is easy to read but not always obvious to implement. In this post, I’m going to go through the Metropolis-Hastings algorithm and show one way of implementing it. There are many ways, some are slower, some are faster, but this is one is mine."
  },
  {
    "objectID": "posts/2025-06-27-mcmc-implementation-jax/index.html#bayesian-statistics-and-mcmc",
    "href": "posts/2025-06-27-mcmc-implementation-jax/index.html#bayesian-statistics-and-mcmc",
    "title": "Metropolis-Hastings in JAX",
    "section": "Bayesian statistics and MCMC",
    "text": "Bayesian statistics and MCMC\nThe set up for Bayesian problems goes something like this… given some observed data we assume it comes from a data generating process (we call this the model), so we want to estimate the parameters of the model. Let \\(\\mathcal{D}\\) be the observed data and \\(\\theta\\) be the model parameters and we can write the following:\n\\[\n\\pi(\\theta \\mid \\mathcal{D}) \\propto p(\\theta) \\, \\mathcal{L}(\\mathcal{D} \\mid \\theta)\n\\]\nWhere \\(p(\\theta)\\) is called the prior distribution (on the model parameters), \\(\\mathcal{L}(\\mathcal{D} \\mid \\theta)\\) is the likelihood function, and \\(\\pi(\\theta \\mid \\mathcal{D})\\) is the posterior. The goal of MCMC is to explore the probability space that is the posteruor distribution. In this post, the posterior distribution is going to be very simple (so much so that you can work it out by hand) but you can imagine that for larger, more complex models, these posterior distributions will not be “nice” to work with. What’s cool about MCMC algorithms, is that they will (eventually) be able to generate samples from these complex posterior distributions so we can find the optimal parameter values for out models."
  },
  {
    "objectID": "posts/2025-06-27-mcmc-implementation-jax/index.html#metropolis-hastings",
    "href": "posts/2025-06-27-mcmc-implementation-jax/index.html#metropolis-hastings",
    "title": "Metropolis-Hastings in JAX",
    "section": "Metropolis-Hastings",
    "text": "Metropolis-Hastings\nThe Metropolis-Hastings (MH) algorithm is a fundamental MCCM method designed to generate samples from complex probability distributions, such as our posterior. It works when direct sampling isn’t possible or when only an unnormalized density function is available. The algorithm goes something like this:\nAt each step \\(t\\), the algorithm proposes a new state \\(\\theta^*\\) from a proposal distribution \\(q(\\theta^*|\\theta_t)\\), which often depends on the current state \\(\\theta_t\\). The proposed state is then accepted with a probability \\(\\alpha\\), known as the Metropolis-Hastings acceptance ratio:\n\\[\n\\alpha(\\theta_t, \\theta^*) = \\min\\left(1, \\frac{\\pi(\\theta^*)q(\\theta_t|\\theta^*)}{\\pi(\\theta_t)q(\\theta^*|\\theta_t)}\\right)\n\\]\nIf the proposed state is accepted, \\(\\theta_{t+1} = \\theta^*\\); otherwise, the chain remains at its current state, \\(\\theta_{t+1} = \\theta_t\\)1. This acceptance criterion is crucial for correcting any bias introduced by the proposal distribution, ensuring that the generated Markov chain has \\(\\pi(\\theta)\\) as its stationary distribution. This outlines a procedure that we can easily write into a for loop; however that style of implementation tends to be slow and cumbersome.\n\n\n\n\n\n\nNote\n\n\n\nIf you’d like to see the algorithm, I’d recommend checking out Scalable Monte Carlo for Bayesian Learning."
  },
  {
    "objectID": "posts/2025-06-27-mcmc-implementation-jax/index.html#data-simulation",
    "href": "posts/2025-06-27-mcmc-implementation-jax/index.html#data-simulation",
    "title": "Metropolis-Hastings in JAX",
    "section": "Data simulation",
    "text": "Data simulation\n\nimport jax\nimport jax.numpy as jnp\nimport distrax\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time \n\nfrom typing import Callable, Tuple\n\nWe need some observed data so we will simulate it from a 1-dimensional Gaussian distribution.\n\nrng_key = jax.random.PRNGKey(20250627) # use a fixed seed for reproducibility\n\n# define the parameters of our *true* data-generating model\n# true_mean is what we are aiming to \"discover\"/estimate through MCMC\ntrue_mean = jnp.array(4.2, dtype=jnp.float32)\ntrue_stddev = jnp.array(1.5, dtype=jnp.float32)\n\n# this model represents the process that generates the observed data.\ndef my_model(mean, stddev):\n    return distrax.Normal(loc=mean, scale=stddev)\n\nmodel = my_model(mean = true_mean, stddev= true_stddev)\n\nobserved_data = model.sample(seed = rng_key, sample_shape=(40,))\n\n\n# plot the observed data - think of this as exploratory data analysis\nplt.figure(figsize=(9,4))\nsns.histplot(observed_data, bins=30, kde=False, stat='density', color='green', label='Observed Data Density', alpha=0.6)\nplt.title('Histogram of Observed Data')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.grid(True, linestyle=':', alpha=0.6)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nProblem statement: given the data above, estimate the mean of the data generating model given the standard deviation is 1.5. \\[\n\\mathcal{D} \\sim N(\\mu, 1.5^2) \\quad \\leftarrow \\text{Find } \\mu\n\\]\n\n\nNow we can create a function that builds our target log-probability function (i.e. the posterior).\n\n\n\n\n\n\nNote\n\n\n\nIn a data science workflow, we won’t know for sure that our model is correct since the data is not simulated. So from here on, we will pretend that we do not know the data is normal, but instead assume that it is.\n\n\n\n# create the flat prior distribution (Normal(0, 10^2) for the parameter mu)\nprior_mean = jnp.array(0.0, dtype=jnp.float32)\nprior_stddev = jnp.array(10.0, dtype=jnp.float32)\n\nprior_distribution = distrax.Normal(loc=prior_mean, scale=prior_stddev)\n\n\ndef make_target_log_prob_fn(prior_dist: distrax.Distribution, \n                                  obs_data: jax.Array, \n                                  obs_likelihood_stddev: jax.Array):\n  \"\"\"\n  Factory function to create a callable for the target log-probability (posterior).\n  This function forms a closure over the prior distribution, observed data,\n  and the likelihood standard deviation.\n\n  Args:\n    prior_dist (distrax.Distribution): The prior distribution for the parameter mu.\n    obs_data (jax.Array): The observed data.\n    obs_likelihood_stddev (jax.Array): The standard deviation for the data likelihood.\n\n  Returns:\n    A callable function `target_log_prob_fn(x)` that returns the unnormalized\n    log-posterior probability of mu given the observed data.\n  \"\"\"\n  @jax.jit # JIT compile the log_prob function\n  def target_log_prob_fn(x):\n    \"\"\"\n    Calculates the unnormalized log-posterior probability for a given parameter value 'x'.\n    This is proportional to log(prior(x)) + log(likelihood(observed_data | x)).\n    \"\"\"\n    # calculate the log-probability of 'x' under the prior distribution\n    log_prior = prior_dist.log_prob(x)\n\n    # define the likelihood distribution for the observed data, assuming 'x' is the mean\n    likelihood_dist = distrax.Normal(loc=x, scale=obs_likelihood_stddev)\n\n    # calculate the log-probability of the observed data under this likelihood\n    # and sum over all data points (assuming independence).\n    log_likelihood = jnp.sum(likelihood_dist.log_prob(obs_data))\n\n    # unnormalized log-posterior is the sum of the log-prior and log-likelihood.\n    return log_prior + log_likelihood\n  return target_log_prob_fn\n\nLet’s test that it works.\n\n# create the specific target_log_prob_callable using our defined prior, observed data, and likelihood stddev.\ntarget_log_prob_fn = make_target_log_prob_fn(\n    prior_distribution, observed_data, true_stddev)\n\n# test the target_log_prob_callable function at a few points.\n# `prior_mean` (0.0) is a good test point for the parameter 'x'.\nprint(f\"Log-posterior at prior mean ({prior_mean.item():.2f}): {target_log_prob_fn(prior_mean).item():.4f}\")\nprint(f\"Log-posterior at 6.0: {target_log_prob_fn(jnp.array(6.0)).item():.4f}\")\n\nLog-posterior at prior mean (0.00): -229.0388\nLog-posterior at 6.0: -101.6034"
  },
  {
    "objectID": "posts/2025-06-27-mcmc-implementation-jax/index.html#a-modular-solution",
    "href": "posts/2025-06-27-mcmc-implementation-jax/index.html#a-modular-solution",
    "title": "Metropolis-Hastings in JAX",
    "section": "A modular solution",
    "text": "A modular solution\nThe goal in this implementation is to “section” off parts of the algorithm that we can allocate to a function that does one thing (a specialist of sorts). The most obvious candidate for this is the proposal function \\(Q(x'|x; \\sigma)\\).\n\nproposal_stddev = jnp.array(1.0, dtype=jnp.float32)\n\ndef make_proposal_distribution_fn(proposal_stddev_val):\n  \"\"\"\n  Factory function to create a callable that generates a Normal proposal distribution.\n\n  Args:\n    proposal_stddev_val (jax.Array): The standard deviation for the proposal distribution.\n\n  Returns:\n    A callable function `proposal_dist_fn(center_val)` that returns a\n    distrax.Normal object centered at `center_val` with\n    the given `proposal_stddev_val`.\n  \"\"\"\n  def proposal_dist_fn(center_val):\n    \"\"\"\n    Creates a Normal distribution centered at `center_val`.\n    \"\"\"\n    return distrax.Normal(loc=center_val, scale=proposal_stddev_val)\n  return proposal_dist_fn\n\n# Create the specific proposal distribution callable using our global standard deviation.\nproposal_distribution_fn = make_proposal_distribution_fn(proposal_stddev)\n\nSince the algorithm requires several probability evaluations, I liked the idea of bundling up these calculations into one function (this can be better implemented by having an individual function for each probability since that would be an easier system to unit test).\n\ndef make_compute_mh_log_probabilities(target_log_prob_fn, proposal_dist_fn):\n    def compute_mh_log_probabilities(current_sample, proposed_sample):\n        \"\"\"\n        Computes all necessary log probabilities for the Metropolis-Hastings acceptance ratio.\n        This function implicitly accesses target_log_prob_fn and proposal_dist_fn from the closure.\n\n        Args:\n            current_sample (jax.Array): The current value in the MCMC chain.\n            proposed_sample (jax.Array): The proposed candidate value.\n\n        Returns:\n            A tuple containing:\n            - proposed_target_log_prob (jax.Array): Log-prob of proposed_sample under target.\n            - current_target_log_prob (jax.Array): Log-prob of current_sample under target.\n            - log_proposal_forward_prob (jax.Array): Log-prob of proposing proposed_sample from current_sample.\n            - log_proposal_reverse_prob (jax.Array): Log-prob of proposing current_sample from proposed_sample.\n        \"\"\"\n        # Calculate the log-probability of the proposed sample under the target distribution.\n        proposed_target_log_prob = target_log_prob_fn(proposed_sample)\n\n        # Calculate the log-probability of the current sample under the target distribution.\n        current_target_log_prob = target_log_prob_fn(current_sample)\n\n        # Define the proposal distribution for the forward step (current -&gt; proposed)\n        proposal_forward_dist = proposal_dist_fn(current_sample)\n        log_proposal_forward_prob = proposal_forward_dist.log_prob(proposed_sample)\n\n        # Define the proposal distribution for the reverse step (proposed -&gt; current)\n        proposal_reverse_dist = proposal_dist_fn(proposed_sample)\n        log_proposal_reverse_prob = proposal_reverse_dist.log_prob(current_sample)\n\n        return (proposed_target_log_prob, current_target_log_prob,\n                log_proposal_forward_prob, log_proposal_reverse_prob)\n        \n    return compute_mh_log_probabilities\n    \ncompute_mh_log_probabilities = make_compute_mh_log_probabilities(\n    target_log_prob_fn, \n    proposal_distribution_fn\n    )\n\nAnd finally for the algorithm!\n\ndef make_metropolis_hastings_step(target_log_prob_fn, proposal_dist_fn):\n  \"\"\"\n  Factory function to create a Metropolis-Hastings step function with a closure\n  over the target log-probability function and the proposal distribution callable.\n\n  Args:\n    target_log_prob_fn (callable): A function that returns the log-probability\n                                    of a sample under the target distribution.\n    proposal_dist_fn (callable): A function `(center_val) -&gt; distrax.Normal`\n                                 that generates the proposal distribution centered at `center_val`.\n\n  Returns:\n    A callable `jax.jit` compiled function that performs one step of the Metropolis-Hastings algorithm.\n  \"\"\"\n  # JIT compile the main step function\n  @jax.jit\n  def metropolis_hastings_step(carry, x): # x is a dummy variable from jax.lax.scan's `xs`\n    \"\"\"\n    Performs one step of the Metropolis-Hastings algorithm.\n    This function now uses the target_log_prob_fn and proposal_dist_fn from its closure.\n\n    Args:\n      carry: A tuple containing (current_sample, current_log_prob, rng_key).\n             - current_sample (jax.Array): The current value in the MCMC chain.\n             - current_log_prob (jax.Array): The log-probability of the current_sample\n                                             under the target distribution.\n             - rng_key (jax.Array): The JAX PRNG key for random operations.\n      x: A dummy variable from the `elems` sequence of `jax.lax.scan` (unused here).\n\n    Returns:\n      A tuple (next_carry, output_value) for `jax.lax.scan`:\n        - next_carry: (next_sample, next_log_prob, updated_rng_key) for the next iteration.\n        - output_value: next_sample (the actual sample to be collected).\n    \"\"\"\n    current_sample, current_log_prob, rng_key = carry\n\n    # Split the RNG key for distinct random operations within this step\n    proposal_key, uniform_key, next_rng_key = jax.random.split(rng_key, 3)\n\n    # 1. Propose a new candidate sample using the `proposal_dist_fn` from closure.\n    # Pass the proposal_key for reproducibility\n    proposed_sample = proposal_dist_fn(current_sample).sample(seed=proposal_key)\n\n    # 2. Compute all necessary log probabilities using our nested helper function.\n    (proposed_target_log_prob, _, # We already have current_log_prob from current_state, so ignore this return\n     log_proposal_forward_prob, log_proposal_reverse_prob) = \\\n        compute_mh_log_probabilities(current_sample, proposed_sample)\n\n    # 3. Calculate the Metropolis-Hastings acceptance ratio in log-space\n    log_acceptance_ratio = (proposed_target_log_prob - current_log_prob) + \\\n                           (log_proposal_reverse_prob - log_proposal_forward_prob)\n\n    # The acceptance ratio 'alpha' must be between 0 and 1.\n    acceptance_ratio = jnp.exp(jnp.minimum(0.0, log_acceptance_ratio))\n\n    # 4. Generate a uniform random number for acceptance check\n    # Pass the uniform_key for reproducibility\n    u = jax.random.uniform(uniform_key, shape=current_sample.shape, dtype=current_sample.dtype)\n\n    # 5. Decide whether to accept the proposed sample\n    accept = jnp.less(u, acceptance_ratio)\n\n    # Select the next sample based on acceptance\n    next_sample = jnp.where(accept, proposed_sample, current_sample)\n\n    # Select the log-probability corresponding to the next sample.\n    next_log_prob = jnp.where(accept, proposed_target_log_prob, current_log_prob)\n\n    # Return the new carry state (for next iteration) and the sample to collect\n    return (next_sample, next_log_prob, next_rng_key), next_sample\n\n  return metropolis_hastings_step"
  },
  {
    "objectID": "posts/2025-06-27-mcmc-implementation-jax/index.html#running-the-algorithm",
    "href": "posts/2025-06-27-mcmc-implementation-jax/index.html#running-the-algorithm",
    "title": "Metropolis-Hastings in JAX",
    "section": "Running the algorithm",
    "text": "Running the algorithm\n\n# Set the total number of samples to generate\nnum_samples = 5000\n\n# Define the initial state (starting point) of our MCMC chain.\ninitial_sample = jnp.array(0.0, dtype=jnp.float32)\ninitial_log_prob = target_log_prob_fn(initial_sample)\n\n# The initial_state (carry) for jax.lax.scan is a tuple of (initial_sample, initial_log_prob, initial_rng_key).\ninitial_carry = (initial_sample, initial_log_prob, rng_key)\n\n\n# Create the specific MH step function (which is already JIT-compiled by its factory)\nmh_step_function = make_metropolis_hastings_step(\n    target_log_prob_fn, proposal_distribution_fn)\n\n\n%%time \nprint(f\"Starting Metropolis-Hastings sampling for {num_samples} steps...\")\n\n# Use jax.lax.scan to run the Metropolis-Hastings steps iteratively.\n# jax.lax.scan returns (final_carry, accumulated_outputs)\nfinal_carry, mh_samples_jax = jax.lax.scan(\n    f=mh_step_function,\n    init=initial_carry,\n    xs=jnp.arange(num_samples) # A dummy sequence of length num_samples to drive the iterations\n)\n\nprint(\"Sampling complete!\")\n\nStarting Metropolis-Hastings sampling for 5000 steps...\nSampling complete!\nCPU times: user 460 ms, sys: 20.5 ms, total: 480 ms\nWall time: 120 ms"
  },
  {
    "objectID": "posts/2025-06-27-mcmc-implementation-jax/index.html#visualizing-the-results",
    "href": "posts/2025-06-27-mcmc-implementation-jax/index.html#visualizing-the-results",
    "title": "Metropolis-Hastings in JAX",
    "section": "Visualizing the results",
    "text": "Visualizing the results\n\nmh_samples = np.array(mh_samples_jax) \n\n\nplt.figure(figsize=(9,4))\nplt.plot(mh_samples, color='blue', alpha=0.6)\n# Plot the true mean of the data-generating process\nplt.axhline(true_mean.item(), color='red', linestyle='--', linewidth = 2, label=f'True Mean (Data Generating) ({true_mean.item():.2f})')\n# Plot the estimated mean from the latter part of the MCMC chain (after some burn-in)\nplt.axhline(np.mean(mh_samples[500:]), color='orange', linestyle='--', linewidth = 2, label=f'Estimated Mean (Post Burn-in) ({np.mean(mh_samples[500:]):.2f})')\nplt.title('Metropolis-Hastings Trace Plot')\nplt.xlabel('Iteration')\nplt.ylabel('Sample Value')\nplt.grid(True, linestyle=':', alpha=0.6)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(9,4))\nsns.histplot(mh_samples, bins=50, kde=True, stat='density', color='green', label='MH Samples Density', alpha=0.6)\n\n# Generate points for the prior PDF\nx_range = np.linspace(np.min(mh_samples) - 1, np.max(mh_samples) + 1, 500)\n# Convert JAX array from distrax.prob to NumPy for plotting\nprior_pdf = np.array(prior_distribution.prob(jnp.array(x_range)))\nplt.plot(x_range, prior_pdf, color='red', linestyle='--', label='Prior PDF')\n\nplt.title('Distribution of MH Samples vs. Prior PDF')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True, linestyle=':', alpha=0.6)\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten() # Flatten the 2x2 array of axes for easy iteration\n\niteration_points = [10, 100, 1000, 5000]\n\n# The true posterior calculations are removed as they are rarely available in practice.\n# We will compare the MCMC estimates against the true data-generating mean.\n# We use the 'true_mean' (4.2) as a reference point for the parameter.\n\nx_range = np.linspace(np.min(mh_samples) - 1, np.max(mh_samples) + 1, 500) # Keep range based on samples\n\n\nfor i, num_iters in enumerate(iteration_points):\n    # Slice the samples up to the current iteration point\n    current_samples = mh_samples[:num_iters]\n\n    ax = axes[i]\n    sns.kdeplot(current_samples, ax=ax, color='blue', fill=True, label='MCMC Estimate')\n    # Plot a vertical line at the true data-generating mean for reference\n    ax.axvline(true_mean.item(), color='red', linestyle='--', label='True Data Mean')\n    ax.set_title(f'KDE after {num_iters} Iterations')\n    ax.set_xlabel('Parameter Value (X)')\n    ax.set_ylabel('Density')\n    ax.legend()\n    ax.grid(True, linestyle=':', alpha=0.6)\n\nplt.suptitle('Evolution of Parameter Distribution with MCMC Iterations', fontsize=16, y=0.98)\nplt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to prevent title overlap\nplt.show()\n\n\n\n\n\n\n\n\nI really like this plot because it shows the intuition behind the algorithm. A well tuned algorithm will slowly converge towards the true underlying parameter value and overpower a flat, uninformative prior.\n\nprint(\"\\n--- Sample Statistics ---\")\nprint(f\"Mean of MH samples: {np.mean(mh_samples):.4f}\")\nprint(f\"Standard deviation of MH samples: {np.std(mh_samples):.4f}\")\n# Use .item() for scalar JAX arrays when printing\nprint(f\"True Mean: {true_mean.item():.4f}\")\n\n# Calculate differences between consecutive samples\ndiffs = np.diff(mh_samples)\n# Count where the difference is not zero (i.e., a new sample was accepted)\naccepted_steps = np.sum(diffs != 0)\n# Acceptance rate is the number of accepted steps divided by total steps (excluding initial)\nacceptance_rate = accepted_steps / (num_samples - 1) # Subtract 1 because diff reduces length by 1\n\nprint(f\"Approximate Acceptance Rate: {acceptance_rate:.4f}\")\n\n\n--- Sample Statistics ---\nMean of MH samples: 4.1865\nStandard deviation of MH samples: 0.2855\nTrue Mean: 4.2000\nApproximate Acceptance Rate: 0.2845\n\n\n\nThanks for reading\nIn my spare time, I like to take photos so I’m going to add one photo I like at the end of each post as a thank you :)"
  },
  {
    "objectID": "posts/2025-06-27-mcmc-implementation-jax/index.html#footnotes",
    "href": "posts/2025-06-27-mcmc-implementation-jax/index.html#footnotes",
    "title": "Metropolis-Hastings in JAX",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want a formal description of the algorithm, you can find it on the Wikepedia page.↩︎"
  },
  {
    "objectID": "posts/2024-07-01-simulating-epidemics/index.html",
    "href": "posts/2024-07-01-simulating-epidemics/index.html",
    "title": "Simulating an epidemic",
    "section": "",
    "text": "When I started my PhD two years ago, I had never looked at an epidemic model outside of an elementry differential equations course. Fast forward two years, and one Covid-19 later, the bulk of my work is centred around epidemic models. More specifically, stochastic epidemic models. Differential equation models are simple and easy to use but are often too rigid since they don’t reflect the randomness we see in the real world when it comes to the spread of disease.\nThis post is going to be a tutorial that I wish I had at the start of my epidemic modelling journey. The aim is to implement/code a chain binomial epidemic model in Python. This model is sometimes referred to as the Reed-Frost model and was initially used to model epidemic spread in the late 1920s (more details1). It is one of the simplest epidemic models as it only focuses on the infection process so take this as the base case. Future posts will dive deeper into the mathematics and how to expand on this model to make it more realistic."
  },
  {
    "objectID": "posts/2024-07-01-simulating-epidemics/index.html#reed-frost-model",
    "href": "posts/2024-07-01-simulating-epidemics/index.html#reed-frost-model",
    "title": "Simulating an epidemic",
    "section": "Reed-Frost model",
    "text": "Reed-Frost model\nThe Reed-Frost model helps us predict the number of people who will become infected over time, given some initial conditions.\n\nIntuition\nImagine a group of people where some are initially infected with a disease, and others are susceptible but not yet infected. The Reed-Frost model works by dividing time into discrete steps2, such as days or weeks. At each time step, each susceptible person has a chance of getting infected based on their contact with infectious individuals.\nAt the beginning of the outbreak, the population is divided into two groups: susceptible (those who can get infected) and infectious (those who are currently infected). If we were to think of this as a graphical representation, it would be a 2 compartment model.\n\n\n\n\n\nflowchart LR\n  A(Susceptible) --&gt; B(Infected)\n\n\n\n\n\n\nNow the question becomes how do individuals move between these states. The model uses a probability parameter to represent the likelihood that a susceptible person will get infected upon contact with an infectious person. This probability is often denoted as \\(p\\).\nThe process unfolds over a series of discrete time steps. At each step, every susceptible individual has a chance to become infected if they come into contact with any infectious individuals (this is a very loose way to apply some mathematical structure to a very complex infection transmission mechanism).\nThe number of new infections at each time step depends on the number of susceptible and infectious individuals and the probability of transmission. The model assumes that once a person is infected, they remain infectious for only one time step.\n\n\nWhy this is a good starting\nThe Reed-Frost model is powerful because it captures the essential dynamics of disease transmission in a straightforward manner. It considers the key factors that drive an epidemic: the number of susceptible individuals, the number of infectious individuals, and the probability of transmission. By iterating this process over multiple time steps, the model can simulate the spread of an infection and help predict its potential impact on the population."
  },
  {
    "objectID": "posts/2024-07-01-simulating-epidemics/index.html#python-implementation",
    "href": "posts/2024-07-01-simulating-epidemics/index.html#python-implementation",
    "title": "Simulating an epidemic",
    "section": "Python implementation",
    "text": "Python implementation\nFor this tutorial, I am going to try to use as few packages as possible so I’m going to restrict myself to only numpy and pandas. The rest of the code will be base Python.\n\n# load packages \nimport numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nsns.set()\n\nLet’s take stock of what we know at this point. The Reed-Frost model which is a 2-state compartmental model.\n\n\n\n\n\n\nImportant\n\n\n\nThis is a class of models and not a specific model.\n\n\nTo specify a model we need to add more structure. We want to know the size of population the disease is spreading through, the duration of each step in which events are happening (recall this is a discrete time model), and the last is going to be the transition rate or epidemic dynamics (how the disease is transmitted). The assumptions are as follows:\n\n100 epidemic units3 with 99 susceptibles and 1 infected units. This is represented as a vector \\([S_0, I_0] = [99,1]\\) (the subscript \\(i\\) is the time period).\nTime period is going to be daily so we set the time delta \\(\\triangle d = 1\\).\nEpidemic dynamics will be a density based transmission where we ‘count’ the number of pairwise interactions between infectious units and susceptibles and divide by the population size. This means that the probabilty of an infection happening has a rate of \\(\\lambda_i= \\beta \\frac{S_i I_i}{S_i + I_i}\\) which translates into a probability of \\(p_i = \\exp\\{- \\lambda_i * \\triangle d \\}\\)4.\n\n\n# initial values\ntime_delta = 1.                 # 1 day \ninitial_pop = [99.,1.]          # population vector \nparameters0 = 0.01              # daily transmission parameter \n\nWith the constants and initial conditions set, its time to move to the main bit of the code which performs the simulation.\n\n\n\n\n\n\nTip\n\n\n\nI’ve been using the term implementing a model and this doesn’t really have a set definition. In this case, implementation is done when you can simulate a correct trajectory of the epidemic but in other settings it may mean fitting a model to data, validating, predicting, etc. The scope of an implementation changes with the nature of the problem and is context specific. Its all jargon here.\n\n\nThe function below called SI_iteration takes 3 parameter values: 1. parameters - this is a vector of model parameters. For now, this is a 1D vector holding the value for \\(\\beta\\). 2. state - this is a 2D vector containing the counts for each of the states. 3. time_delta - the size (in days) of the discrete time step\n\ndef SI_iteration(parameters, state, time_delta):\n  beta = parameters\n  num_susceptibles, num_infected = state\n  \n  # Step 1: compute transition rates \n1  SI_rate = beta*(num_susceptibles * num_infected)/(num_susceptibles + num_infected)\n\n  # Step 2: convert event rates to probabilities \n2  i_rate = 1 - np.exp(- SI_rate * time_delta)\n\n  # Step 3: simulate number of new infections \n3  num_new_infections = np.random.binomial(num_susceptibles, i_rate)\n\n  # Step 4; state update\n  return [num_susceptibles - num_new_infections, num_infected + num_new_infections]\n\n\n1\n\nCompute the transition rate\n\n2\n\nTurn the transition rate into a probability of new infections\n\n3\n\nSample the number of new infections\n\n\n\n\nI’m going to ignore the mathematics of Step 2 in chunk above for now since that will be its own post down the line. The gory details can be found in the paper I mentioned earlier for those curious. Now for using our new function! Taking the initial values for each of the arguments and plugging them in, we can get the outcome of a single iteration or single step of our epidemic process. This is not a complete path!\n\n# single iteration/forward step\n1np.random.seed(20240701)\nnew_state = SI_iteration(parameters0, initial_pop, time_delta)\n\nprint(f'New state: {new_state}')\n\n\n1\n\nSet the seed for reproducible value.\n\n\n\n\nNew state: [98.0, 2.0]\n\n\n\nSimulate a path\nWe can extend the single iteration to sample a full path of the epidemic by calling our function over and over again. This is not the most efficient way of performing a simulation but it is easy to understand so it’ll do for now.\n\n# simulate full path\ndef simulate(initial_state, num_steps):\n    # counters\n    ii = 0\n    S = np.zeros(num_steps)\n    I = np.zeros(num_steps)\n\n    # unpack the state\n    S[0], I[0] = initial_state\n    state = initial_state\n\n    while ii &lt; num_steps:\n      # single iteration to update the state\n      state = SI_iteration(parameters0, state, time_delta)\n      # record the outcome\n      S[ii], I[ii] = state\n\n      ii += 1\n\n    return {'Time': np.cumsum(np.repeat(time_delta, num_steps)),\n            'Susceptible': S,\n            'Infected': I}\n\nAnd run a simulation of 100 steps (note that some of the steps might be non-events since every unit has progressed through the epidemic). There are nice ways of stopping simulations early by using a clever stop condition within the while loop.\n\nsample_epi = simulate(initial_pop, 100)\n\npd.DataFrame(sample_epi).head(10)\n\n\n\n\n\n\n\n\nTime\nSusceptible\nInfected\n\n\n\n\n0\n1.0\n99.0\n1.0\n\n\n1\n2.0\n97.0\n3.0\n\n\n2\n3.0\n94.0\n6.0\n\n\n3\n4.0\n86.0\n14.0\n\n\n4\n5.0\n74.0\n26.0\n\n\n5\n6.0\n56.0\n44.0\n\n\n6\n7.0\n41.0\n59.0\n\n\n7\n8.0\n35.0\n65.0\n\n\n8\n9.0\n29.0\n71.0\n\n\n9\n10.0\n21.0\n79.0\n\n\n\n\n\n\n\nLets plot the simulation!\n\nplt.figure(figsize=(12, 7))\n\nS_line = plt.plot(\"Time\",\"Susceptible\", data=sample_epi, color=\"b\", linewidth=2)\nI_line = plt.plot(\"Time\",\"Infected\", data=sample_epi, color=\"r\", linewidth=2)\n\nplt.xlabel(\"Days\",fontweight=\"bold\")\nplt.ylabel(\"Count\",fontweight=\"bold\")\n\nlegend = plt.legend(title=\"Population\",loc=5,bbox_to_anchor=(1.2,0.5))\nframe = legend.get_frame()\nframe.set_facecolor(\"white\")\nframe.set_linewidth(0)\n\n# Add labels and title\nplt.xlabel('Days', fontweight=\"bold\")  # Add x-axis label with font styling\nplt.ylabel('Count', fontweight=\"bold\")  # Add y-axis label with font styling\nplt.title('Single path simulation', fontsize=16)  # Add plot title with font size\n\nplt.show()\n\n\n\n\nFull path of an epidemic. Simulation is done by calling the single iteration function repeatedly\n\n\n\n\n\n\nConclusion\nIn this tutorial, I showed one way to implement a chain binomial epidemic model. This basic model is a great starting point for building more complex and realistic models. The function SI_iteration uses a simplified version of the Gillespie algorithm which is the standard algorithm for simulating stochastic trajectories for these types of systems. I’d also recommend checking out this blog post by Lewis Cole for an in depth look at the algorithm.\n\n\nNext steps\nAs far as using epidemic models in the real world goes, simulation is only one half of the coin. The other side has to do with fitting these models. If the problem was reversed and we had observed an epidemic process, recorded the data and created a data set that looks something like @epi_dataset we could then try to find the value of parameters which generated that data. This can be done in a variety of ways such as maximum likelihood estimation or Bayesian inference. If this type of problem interests you, I’d highly recommend checking out the IDDinf 2024 inference course. This is a course I will be co-teaching with some incredible people in September 2024 and will go over everything you need to know to fit epidemic models with Bayesian inference.\n\n\nThanks for reading\nIn my spare time, I like to take photos so I’m going to add one photo I like at the end of each post as a thank you :)"
  },
  {
    "objectID": "posts/2024-07-01-simulating-epidemics/index.html#footnotes",
    "href": "posts/2024-07-01-simulating-epidemics/index.html#footnotes",
    "title": "Simulating an epidemic",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nResource here↩︎\nI usually work with continuous time models which pose their own challenges. More on this will come in later posts↩︎\nAn epidemic unit can be anything that we can classify as susceptible or infected. These models are used for humans and animals so its easier to generalize the concept of an epidemic unit. The units can even be aggregated to represent households or farms. It gives us a more flexibility to adapt the scope of the model.↩︎\nFuture post on the interpretable mathematics of epidemic models↩︎"
  },
  {
    "objectID": "posts/2024-09-30-conditional-probs-partial-evals/index.html",
    "href": "posts/2024-09-30-conditional-probs-partial-evals/index.html",
    "title": "Conditional probabilities are partial function applications",
    "section": "",
    "text": "During my undergraduate degree, I always struggled with the concept of computing statistical quantities. I couldn’t wrap my head around what commands to use in my editor to mimic the math that I could write on the page. Fast forward a several years and lots of coding assignments, I now kind of get it. I’m writing this blog post to highlight one of the nicest links between the math and the computation that I’ve come across and the motivation behind the title of this post - conditional probabilities are partial function applications."
  },
  {
    "objectID": "posts/2024-09-30-conditional-probs-partial-evals/index.html#quick-refresher-on-conditional-probabilities",
    "href": "posts/2024-09-30-conditional-probs-partial-evals/index.html#quick-refresher-on-conditional-probabilities",
    "title": "Conditional probabilities are partial function applications",
    "section": "Quick refresher on conditional probabilities",
    "text": "Quick refresher on conditional probabilities\nLet’s start with a quick recap of what exactly a conditional probability is. Suppose we have 2 events, \\(A\\) and \\(B\\). We say that the conditional probability of B given A is \\[P(B \\vert A) = \\frac{P(A \\bigcap B)}{P(A)}\\] We assume that \\(P(A)&gt;0\\). The intuition here is that we are updating our belief about \\(B\\) knowing that something specific about \\(A\\) has occured. If these events are independent, then the above equation simplifies since we \\(P(A \\bigcap B) = P(A) P(B)\\) (i.e. knowing something about \\(A\\) gives you no additional information about \\(B\\)).\nHowever, conditional probability extends far beyond simple events. In statistical modeling, conditional probabilities form the backbone of likelihood-based inference. When we model data, we often use conditional probabilities to express how likely the observed data is, given a set of model parameters (data generating model). This is where the likelihood function comes into play. The likelihood function is an unnormalized probability distribution that is a function of the model parameters, not the data. It captures the plausibility of the parameters given the data, and by conditioning on different parameters, we can explore various hypotheses or refine our model.\nFor example, in Bayesian inference, we condition on observed data to update our prior beliefs about the model parameters, yielding the posterior distribution. This conditional framework underpins nearly all of modern statistical inference, from maximum likelihood estimation to more complex Markov Chain Monte Carlo inference schemes. These likelihood functions can be very easy to write on paper but difficult to code and that’s what I want to dive into here. Let’s define a model with two parameters. \\[y_i \\sim N(\\beta_0 + \\beta_1 x_i, 2^2)\\] Some may recognize this as a linear regression. Our dependent random varialbe \\(y\\) follows a Normal distribution which has a mean that depends on a linear transformation of the independent variable \\(x\\). We can write out the likelihood function of this model using the probability density function of the Normal. \\[\\begin{align}\nL(\\beta_0, \\beta_1 ; x) &= \\prod_{i = 1}^n P(y_i) \\\\\n&= \\prod_{i = 1}^n \\frac{1}{\\sqrt{2 \\pi \\cdot0.52^2}} e^{-\\frac{(y_i - (\\beta_0 + \\beta_1 x_i))^2}{2 \\cdot 0.5^2}}\n\\end{align}\\] Here’s a simulated data set from this model.\n\n# parameter values \nbeta0 = 1.2\nbeta1 = 0.5\n\n# random x values\nx = np.random.uniform(\n    low = 0.0, \n    high = 10.0,\n    size = 100,\n    )\n# evaluate mean\nmu = beta0 + beta1*x\n# simulate y values \ny = np.random.normal(\n    loc = mu,\n    scale = 0.5, \n    size = 100\n    )\n\n\n# Scatter plot of the data\nplt.scatter(x, y, label=\"Data points\", color='blue')"
  },
  {
    "objectID": "posts/2024-09-30-conditional-probs-partial-evals/index.html#partial-function-applications",
    "href": "posts/2024-09-30-conditional-probs-partial-evals/index.html#partial-function-applications",
    "title": "Conditional probabilities are partial function applications",
    "section": "Partial function applications",
    "text": "Partial function applications\nIn Python, partial function applications is a feature provided by functools package with the partial function. It allows you to take a function and fix some of its arguments, returning a new function that takes fewer arguments. The similarity to conditional probabilities lies in this idea of fixing known inputs. This is a nice feature of Python - we can pass around functions as first class objects (an entity that can be dynamically created, destroyed, passed to a function, returned as a value, and have all the rights as other variables in the programming language have).\nLet’s look at an example.\n\nfrom functools import partial \n\n\ndef my_function(a, b):\n    \"\"\"\n    A function that takes two arguements and returns the product\n    \"\"\"\n    return a*b\n\nWe can partially apply the function by “fixing” one of the parameters values to a specific value. For example, fix a=2.\n\nmultiply_by_2 = partial(my_function, a = 2)\n\nprint(multiply_by_2)                        # this returns a function\nprint(multiply_by_2(b = 4))                 # this returns 8\n\nfunctools.partial(&lt;function my_function at 0x14f4972e0&gt;, a=2)\n8\n\n\nHere we used partial to create a new function that fixes one of the arguments to 2, hence the new function is multiplies the input by 2. If the arguments to the my_function correspond to parameter values in my model and the output is the likelihood function, then applying partial returns the conditional likelihood."
  },
  {
    "objectID": "posts/2024-09-30-conditional-probs-partial-evals/index.html#why-does-this-analogy-work",
    "href": "posts/2024-09-30-conditional-probs-partial-evals/index.html#why-does-this-analogy-work",
    "title": "Conditional probabilities are partial function applications",
    "section": "Why does this analogy work?",
    "text": "Why does this analogy work?\nThe analogy between partial function applications and conditional probability is very nice because both involve reducing complexity by “conditioning” on known information. In statistics, we’re conditioning on known events/realizations, while in programming, we’re fixing known inputs/data.\nConsider this: when you fix one argument of a function, you’re effectively “conditioning” the function on that known value. Similarly, conditional probability refines the likelihood of an event by fixing certain known information. This way of thinking can be particularly powerful when designing simulations or modeling problems where we frequently update our beliefs based on new information."
  },
  {
    "objectID": "posts/2024-09-30-conditional-probs-partial-evals/index.html#apply-it-to-our-model",
    "href": "posts/2024-09-30-conditional-probs-partial-evals/index.html#apply-it-to-our-model",
    "title": "Conditional probabilities are partial function applications",
    "section": "Apply it to our model",
    "text": "Apply it to our model\nLets create the likelihood function of our model. I’m going to take advantage of the logpdf method of the normal distribution which ‘scipy’ has already implemented. It’s always good practice to allocate these computations to well tested and documented libraries so we avoid any algebraic mistakes in our code.\n\ndef model_likelihood(beta0, beta1):\n    log_probs = sp.stats.norm(\n        loc = mu,           # model mean - computed as global variable earlier\n        scale = 0.5         # fixed variance\n        ).logpdf(y)         # log prob of observed random variables \n    return np.sum(log_probs)\n\n# test arbitrary values\nprint(f\"log-prob eval: {model_likelihood(beta0 = 0.1, beta1 = 0.1)}\")\n\nlog-prob eval: -72.04687255986312\n\n\nNow we can use the partial to get the conditional likelihood of beta0 given beta1.\n\nconditional_likelihood = partial(model_likelihood, beta1 = 0.1)\n\nprint(f\"Conditional log-likelihood: {conditional_likelihood}\")\nprint(f\"Check: {conditional_likelihood(beta0= 0.1)}\")\n\nConditional log-likelihood: functools.partial(&lt;function model_likelihood at 0x14f4971a0&gt;, beta1=0.1)\nCheck: -72.04687255986312\n\n\nAnd they are the same as before! This mirrors how we compute conditional probabilities by progressively refining our estimate as more information becomes available. For example, if you found some information saying that beta1 should be 0.5, you can fix that using this technique. All you need to do then is optimize the (log) likelihood for the other parameter."
  },
  {
    "objectID": "posts/2024-09-30-conditional-probs-partial-evals/index.html#closing-thoughts",
    "href": "posts/2024-09-30-conditional-probs-partial-evals/index.html#closing-thoughts",
    "title": "Conditional probabilities are partial function applications",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nThe parallel between partial function applications and conditional probabilities provides an intuitive bridge between coding and probability theory. By conditioning on known values, both in probability and programming, we can simplify complex systems and gain clearer insights into the behavior of the remaining uncertainties.\nIn your next coding or probability problem, try thinking of how partial applications might represent conditioned states of knowledge. This perspective can make otherwise complex ideas feel a bit more manageable—and highlight the deep connections between computation and probability theory.\n\nThanks for reading\nIn my spare time, I like to take photos so I’m going to add one photo I like at the end of each post as a thank you :)"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Talks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom code banks to software\n\n\n\n\n\nExploring the lifeline of a coding project and key decisions developers must make along the way\n\n\n\n\n\nJan 31, 2024\n\n\nAlin Morariu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/index_talk_template.html",
    "href": "talks/index_talk_template.html",
    "title": "Talk title",
    "section": "",
    "text": "Your abstract"
  },
  {
    "objectID": "talks/index_talk_template.html#abstract",
    "href": "talks/index_talk_template.html#abstract",
    "title": "Talk title",
    "section": "",
    "text": "Your abstract"
  },
  {
    "objectID": "talks/index_talk_template.html#slides",
    "href": "talks/index_talk_template.html#slides",
    "title": "Talk title",
    "section": "Slides",
    "text": "Slides\n\n\n\n\n\n Full Screen"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a PhD student at Lancaster University with an interest in Bayesian statistics and all the computational aspects that come along with that. This includes inference algorithms like MCMC, probablistic programming, efficient implementations, and software for making this more accessible. In the past, I’ve worked as a data scientist and analyst in the finance industry as well as a researcher in a hospital.\n\n\nLancaster University | Lancaster, UK PhD in Statistics | Jan 2022 - Dec 2025\nToronto Metropolitan Univeristy | Toronto, Canada MSc in Applied Mathematics | Sept 2019 - June 2021\nUniversity of Toronto, St George | Toronto, Canada HBSc in Statistics and Mathematics | Sept 2014 - June 2019\n\n\n\nMy previous work experience can be found on my resume or my LinkedIn"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "Lancaster University | Lancaster, UK PhD in Statistics | Jan 2022 - Dec 2025\nToronto Metropolitan Univeristy | Toronto, Canada MSc in Applied Mathematics | Sept 2019 - June 2021\nUniversity of Toronto, St George | Toronto, Canada HBSc in Statistics and Mathematics | Sept 2014 - June 2019"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "",
    "text": "My previous work experience can be found on my resume or my LinkedIn"
  },
  {
    "objectID": "about.html#get-in-touch",
    "href": "about.html#get-in-touch",
    "title": "About",
    "section": "Get in touch!",
    "text": "Get in touch!\nBest ways of contacting me is through email!\n\\(\\rightarrow\\) a.morariu@lancaster.ac.uk\\(\\leftarrow\\)"
  }
]